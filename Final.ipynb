{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0UU9NVl7jiW"
   },
   "source": [
    "#### IMPORTING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQDcZiHw7jio",
    "outputId": "2531908f-dc4c-4147-e6a0-25d62e693d53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wazir\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Wazir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wazir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from tensorflow.keras.models import load_model\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Attention\n",
    "#import fasttext as ft\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import mean_squared_log_error as msle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wkQcG0iTt11"
   },
   "source": [
    "#### FINAL FUNCTION FOR PREDICTION\n",
    "\n",
    "This function would take a single data point and will predict the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2Gnqc_8M7jiq"
   },
   "outputs": [],
   "source": [
    "def final(X):    \n",
    "    \n",
    "    ''' Defining the function decontracted which expands anything with apostrophes'''\n",
    "    def decontracted(phrase):\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    \n",
    "    ''' Defining the function preprocess to preprocess the product name, item_description and brand_name'''\n",
    "    def preprocess(sentence):\n",
    "        # Converting the sentence to a string instance\n",
    "        sentence = str(sentence)\n",
    "        sent = decontracted(sentence)\n",
    "        sent = sent.replace('\\\\r',' ')\n",
    "        sent = sent.replace('\\\\t',' ')\n",
    "        sent = sent.replace('\\\\\"',' ')\n",
    "        sent = sent.replace('\\\\n',' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+',' ',sent)\n",
    "        sent = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+',' ',sent) # Removing the punctuations\n",
    "        sent = ' '.join(e for e in sent.split() if e.lower() not in nltk.corpus.stopwords.words('english') and len(e)>=3)\n",
    "        return sent.lower().strip()\n",
    "\n",
    "    ''' Importing the unique training brand list'''\n",
    "    training_brands = pd.read_csv(\"training_brands.txt\",sep = '\\t')  \n",
    "\n",
    "    '''Initializing the unique training brand list'''\n",
    "    unique_brand_set = list(training_brands.brands.values)\n",
    "\n",
    "    '''Defining the function to replace the brand_name from the unique list of brand names'''\n",
    "    def brand_name_replace(brand_name,product_desc):\n",
    "      if product_desc.split()[0] in unique_brand_set: # Checking whether the first word of the description is a valid brand\n",
    "        brand_name = product_desc.split()[0]\n",
    "      elif len(product_desc.split()) >= 2 and ' '.join(product_desc.split()[0:2]) in unique_brand_set: # Checking whether the first two words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:2])\n",
    "      elif len(product_desc.split()) >= 3 and ' '.join(product_desc.split()[0:3]) in unique_brand_set: # Checking whether the first three words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:3])\n",
    "      elif len(product_desc.split()) >= 4 and ' '.join(product_desc.split()[0:4]) in unique_brand_set: # Checking whether the first four words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:4])\n",
    "      elif len(product_desc.split()) >= 5 and ' '.join(product_desc.split()[0:5]) in unique_brand_set: # Checking whether the first five words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:5])\n",
    "      elif len(product_desc.split()) >= 6 and ' '.join(product_desc.split()[0:6]) in unique_brand_set: # Checking whether the first six words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:6])\n",
    "      elif len(product_desc.split()) >= 7 and ' '.join(product_desc.split()[0:7]) in unique_brand_set: # Checking whether the first seven words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:7])\n",
    "      elif len(product_desc.split()) >= 8 and ' '.join(product_desc.split()[0:8]) in unique_brand_set: # Checking whether the first eight words of the description is a valid brand\n",
    "        brand_name = ' '.join(product_desc.split()[0:8])\n",
    "      return brand_name\n",
    "\n",
    "    '''Defining the function to extract the item description length'''\n",
    "    def count_of_words(text):\n",
    "        '''This function would remove the punctuations, numbers and \n",
    "        stopwords from the text and then converting everything to lowercase\n",
    "        and thereby returning the number of words in the text'''\n",
    "        try:\n",
    "            text = text.replace('\\\\t','') # Removing the tabs\n",
    "            text = text.replace('\\\\r','') # Removing the \\r\n",
    "            text = text.replace('\\\\n','') # Removing the newline character\n",
    "            text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+','',text) # Removing the punctuations\n",
    "            text = re.sub('[0-9]+','',text) # Removing the numbers\n",
    "            # Capturing the clean text in a string and then returning the length of the list for words greater than length of 3        \n",
    "            new_text = ' '.join(word for word in text.split() if word.lower() not in nltk.corpus.stopwords.words('english') and len(word)>3)\n",
    "            # Returning the length of the words in the string new_text\n",
    "            return len(new_text.split())        \n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    '''Defining the function to calculate the sentiment score of the preprocessed item description'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    def sentiment_score(sentence):\n",
    "        ss = sid.polarity_scores(str(sentence))\n",
    "        return ss\n",
    "    \n",
    "    '''Defining the function to separate out the category names'''\n",
    "    # Defining a function to separate out the three sub categories\n",
    "\n",
    "    def category_split(category):\n",
    "        '''This function separates out 3 particular sub categories from a category name'''\n",
    "\n",
    "        # Defining a try-except block to check whether the category can be splitted into sub categories or not. The except block\n",
    "        # catches the exception if there is no '/' in the category name and then returns a list with strings No Label repeated 3\n",
    "        # times\n",
    "        try:\n",
    "            return category.split('/')\n",
    "        except:\n",
    "            return (\"No Label\",\"No Label\",\"No Label\")\n",
    "    \n",
    "    '''Preprocessing the product name'''\n",
    "    preprocessed_name = preprocess(X[0])\n",
    "    \n",
    "    '''Preprocessing the brand_name'''\n",
    "    if X[3] is np.nan:\n",
    "        brand_name = brand_name_replace(X[3],X[0])\n",
    "        if brand_name is np.nan:\n",
    "          print(\"No Brands present for this product\")\n",
    "          return 0\n",
    "        preprocessed_brand_nm = preprocess(brand_name)\n",
    "    else:\n",
    "        preprocessed_brand_nm = preprocess(X[3])\n",
    "        \n",
    "    '''Preprocessing the categories after getting the individual categories which are general_cat, subcat_1 and subcat_2'''\n",
    "    general_cat, subcat_1, subcat_2 = category_split(X[2])\n",
    "    \n",
    "    '''Preprocessing the item description'''\n",
    "    preprocessed_item_desc = preprocess(X[5])\n",
    "    \n",
    "    '''Getting the item_description length of the preprocessed item description'''\n",
    "    item_desc_length = count_of_words(preprocessed_item_desc)\n",
    "    \n",
    "    '''Preprocessing the general_cat'''\n",
    "    general_cat = general_cat.replace(\" & \",\"_\")\n",
    "    general_cat = general_cat.replace(\" \",\"_\")\n",
    "    general_cat = general_cat.lower()\n",
    "    \n",
    "    '''Preprocessing the subcat_1'''\n",
    "    subcat_1 = subcat_1.replace(\" & \",\"_\")\n",
    "    subcat_1 = subcat_1.replace(\" \",\"_\")\n",
    "    subcat_1 = subcat_1.replace(\"-\",\"_\")\n",
    "    subcat_1 = subcat_1.replace(\"\\'s\",\"\")\n",
    "    subcat_1 = subcat_1.replace(\"\\(\",\"\")\n",
    "    subcat_1 = subcat_1.replace(\"\\)\",\"\")\n",
    "    subcat_1 = subcat_1.lower()\n",
    "    \n",
    "    '''Preprocessing the subcat_2'''\n",
    "    subcat_2 = subcat_2.replace(\" & \",\"_\")\n",
    "    subcat_2 = subcat_2.replace(\" \",\"_\")\n",
    "    subcat_2 = subcat_2.replace(\"-\",\"_\")\n",
    "    subcat_2 = subcat_2.replace(\"\\'s\",\"\")\n",
    "    subcat_2 = subcat_2.replace(\"\\(\",\"\")\n",
    "    subcat_2 = subcat_2.replace(\"\\)\",\"\")\n",
    "    subcat_2 = subcat_2.lower()\n",
    "    \n",
    "    '''Concatenating the preprocessed_name and preprocessed_item_desc'''\n",
    "    name_item_desc = preprocessed_name + \" \" + preprocessed_item_desc\n",
    "    \n",
    "    '''Concatenating the preprocessed_brand_nm, general_cat, subcat_1 and subcat_2'''\n",
    "    concatenated_brcat = preprocessed_brand_nm + \" \" + general_cat + \" \" + subcat_1 + \" \" + subcat_2\n",
    "    \n",
    "    '''Getting the sentiment score of the preprocessed_item_desc'''\n",
    "    sent_sc = sentiment_score(str(preprocessed_item_desc))\n",
    "    \n",
    "    '''Concatenating the numerical features'''\n",
    "    numerical_features = [X[1]] + [X[4]] + [item_desc_length] + [v for v in sent_sc.values()]\n",
    "    \n",
    "    '''Loading the tokenizers'''\n",
    "    Tokenizer_pritem_desc = joblib.load('Tokenizer_pritem_desc.pkl')\n",
    "    Tokenizer_brcat = joblib.load('Tokenizer_brcat.pkl')\n",
    "    \n",
    "    '''Loading the minmaxscaler'''\n",
    "    scaler = joblib.load('minmaxscaler.pkl')\n",
    "    \n",
    "    '''Loading the embedding matrix'''\n",
    "    embedding_matrix = np.load('embedding_matrix.npy')\n",
    "    \n",
    "    ''' Defining the model variables'''\n",
    "    maxlen_pritem_desc = 154\n",
    "    maxlen_brcat = 8\n",
    "    \n",
    "    ''' Defining the model architecture'''\n",
    "    embedding_dim_brcat = 50\n",
    "    num_tokens_brcat = len(Tokenizer_brcat.word_index)+1\n",
    "    embedding_dim_preprocessed_pritemdesc = 300\n",
    "    num_tokens_preprocessed_pritemdesc = len(Tokenizer_pritem_desc.word_index)+1\n",
    "\n",
    "    # Creating the model architecture\n",
    "    Inp1 = Input(shape = (maxlen_pritem_desc,), dtype='int64')\n",
    "    Emb1 = Embedding(input_dim = num_tokens_preprocessed_pritemdesc, output_dim = embedding_dim_preprocessed_pritemdesc, input_length = maxlen_pritem_desc,\n",
    "                    embeddings_initializer = tf.keras.initializers.constant(embedding_matrix), trainable = False)(Inp1)\n",
    "    LSTM_layer_1 = LSTM(units=50, return_sequences = True)(Emb1)\n",
    "\n",
    "    #avgpool = AveragePooling1D(pool_size = 2, strides=2)(Emb1)\n",
    "    #Flatten_1 = Flatten()(avgpool)\n",
    "\n",
    "    Inp2 = Input(shape = (maxlen_brcat,), dtype='int64')\n",
    "    Emb2 = Embedding(input_dim = num_tokens_brcat, output_dim = embedding_dim_brcat, input_length = maxlen_brcat,trainable = True)(Inp2)\n",
    "    LSTM_layer_2 = LSTM(units=50, return_sequences = True)(Emb2)\n",
    "    #Flatten_2 = Flatten()(Emb2)\n",
    "\n",
    "    att_contextvec = Attention()([LSTM_layer_2,LSTM_layer_1])\n",
    "\n",
    "    avgpool = GlobalAveragePooling1D()(att_contextvec)\n",
    "\n",
    "    Flatten_1 = Flatten()(avgpool)\n",
    "\n",
    "    Inp3 = Input(shape = (7,))\n",
    "    Dense_1 = Dense(units = 4, activation = 'relu', kernel_initializer = 'he_normal')(Inp3)\n",
    "    #Dense_2 = Dense(units = 4, activation = 'relu', kernel_initializer = 'he_uniform')(Dense_2)\n",
    "\n",
    "    concat = concatenate([Flatten_1,Dense_1])\n",
    "\n",
    "    BN_1 = BatchNormalization()(concat)\n",
    "\n",
    "    Dense_2 = Dense(units = 16, kernel_initializer = 'he_normal')(BN_1)\n",
    "\n",
    "    Dense_2 = tf.keras.layers.PReLU()(Dense_2)\n",
    "\n",
    "    dropout_1 = Dropout(0.5)(Dense_2)\n",
    "\n",
    "    #BN_2 = BatchNormalization()(dropout_1)\n",
    "\n",
    "    Dense_3 = Dense(units = 8, kernel_initializer = 'he_normal')(dropout_1)\n",
    "\n",
    "    Dense_3 = tf.keras.layers.PReLU()(Dense_3)\n",
    "\n",
    "    dropout_2 = Dropout(0.4)(Dense_3)\n",
    "\n",
    "    Output = Dense(units = 1, activation = 'relu')(dropout_2)\n",
    "\n",
    "    model = Model(inputs = [Inp1, Inp2, Inp3], outputs = [Output])\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    '''Loading the best model weights'''\n",
    "    model.load_weights('best_model_weights.hdf5')\n",
    "    \n",
    "    '''Scaling the numerical features'''\n",
    "    numerical_features_scaled = scaler.transform(np.array(numerical_features).reshape(1,-1))\n",
    "    \n",
    "    '''Generating the padded sequence for the concatenated product name and preprocessed_item_desc'''\n",
    "    pritem_desc_sequence = Tokenizer_pritem_desc.texts_to_sequences([name_item_desc])\n",
    "    pritem_desc_padded_sequence = pad_sequences(pritem_desc_sequence, padding = 'post', truncating = 'post', maxlen = 154)\n",
    "    \n",
    "    '''Generating the padded sequence for the concatenated brand name and the categories'''\n",
    "    brcat_sequence = Tokenizer_brcat.texts_to_sequences([concatenated_brcat])\n",
    "    brcat_padded_sequence = pad_sequences(brcat_sequence, padding = 'post', truncating = 'post', maxlen = 8)\n",
    "    \n",
    "    '''Predicting on the test instance'''\n",
    "    prediction = model.predict([np.array(pritem_desc_padded_sequence).reshape(1,-1), np.array(brcat_padded_sequence).reshape(1,-1), numerical_features_scaled.reshape(1,-1)])\n",
    "    \n",
    "    '''Returning the predicted price'''\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6LuWrHNd7ji8",
    "outputId": "f93a8acc-2690-4c99-e3d8-6d881e4295f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Breast cancer \"I fight like a girl\" ring</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Size 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers</td>\n",
       "      <td>1</td>\n",
       "      <td>Other/Office supplies/Shipping Supplies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coach bag</td>\n",
       "      <td>1</td>\n",
       "      <td>Vintage &amp; Collectibles/Bags and Purses/Handbag</td>\n",
       "      <td>Coach</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new coach bag. Bought for [rm] at a Coac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Floral Kimono</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Sweaters/Cardigan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-floral kimono -never worn -lightweight and pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Life after Death</td>\n",
       "      <td>3</td>\n",
       "      <td>Other/Books/Religion &amp; Spirituality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Rediscovering life after the loss of a loved o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                      name  item_condition_id  \\\n",
       "0        0  Breast cancer \"I fight like a girl\" ring                  1   \n",
       "1        1  25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers                  1   \n",
       "2        2                                 Coach bag                  1   \n",
       "3        3                             Floral Kimono                  2   \n",
       "4        4                          Life after Death                  3   \n",
       "\n",
       "                                    category_name brand_name  shipping  \\\n",
       "0                             Women/Jewelry/Rings        NaN         1   \n",
       "1         Other/Office supplies/Shipping Supplies        NaN         1   \n",
       "2  Vintage & Collectibles/Bags and Purses/Handbag      Coach         1   \n",
       "3                         Women/Sweaters/Cardigan        NaN         0   \n",
       "4             Other/Books/Religion & Spirituality        NaN         1   \n",
       "\n",
       "                                    item_description  \n",
       "0                                             Size 7  \n",
       "1  25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined...  \n",
       "2  Brand new coach bag. Bought for [rm] at a Coac...  \n",
       "3  -floral kimono -never worn -lightweight and pe...  \n",
       "4  Rediscovering life after the loss of a loved o...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test_stg2.tsv\", sep=\"\\t\")\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPe1jr5h7jjJ",
    "outputId": "dea23b75-a07e-4566-9703-3c0e79d45ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Brands present for this product\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_prediction = final(list(test_data.iloc[4,1:]))\n",
    "end_time = time.time()\n",
    "\n",
    "diff = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7hLUhUMSrUL",
    "outputId": "02ca03fb-47b1-40cb-b477-dbfde88245b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction of price is 0  in 0.23447561264038086 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"The prediction of price is {}  in {} seconds\".format(test_prediction, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brand new coach bag. Bought for [rm] at a Coach outlet.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[2,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINAL FUNCTION FOR CALCULATING ERROR METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_metric(X,Y):\n",
    "    ''' Defining the function decontracted which expands anything with apostrophes'''\n",
    "    def decontracted(phrase):\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    \n",
    "    ''' Defining the function preprocess to preprocess the product name, item_description and brand_name'''\n",
    "    def preprocess(text_array):\n",
    "        # Initializing an empty list named preprocessed_total_train\n",
    "        preprocessed = []\n",
    "\n",
    "        for sentence in tqdm(text_array):\n",
    "            sentence = str(sentence)\n",
    "            sent = decontracted(sentence)\n",
    "            sent = sent.replace('\\\\r',' ')\n",
    "            sent = sent.replace('\\\\t',' ')\n",
    "            sent = sent.replace('\\\\\"',' ')\n",
    "            sent = sent.replace('\\\\n',' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+',' ',sent)\n",
    "            sent = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+',' ',sent) # Removing the punctuations\n",
    "            sent = ' '.join(e for e in sent.split() if e.lower() not in nltk.corpus.stopwords.words('english') and len(e)>=3)\n",
    "            preprocessed.append(sent.lower().strip())\n",
    "        return preprocessed\n",
    "\n",
    "    ''' Importing the unique training brand list'''\n",
    "    training_brands = pd.read_csv(\"training_brands.txt\",sep = '\\t')  \n",
    "\n",
    "    '''Initializing the unique training brand list'''\n",
    "    unique_brand_set = list(training_brands.brands.values)\n",
    "\n",
    "    '''Defining the function to replace the brand_name from the unique list of brand names'''\n",
    "    def brand_name_replace(brand_name,product_desc):\n",
    "        for i in range(len(brand_name)):\n",
    "            if brand_name[i] is np.nan:\n",
    "                if product_desc[i].split()[0] in unique_brand_set: # Checking whether the first word of the description is a valid brand\n",
    "                    brand_name[i] = product_desc[i].split()[0]\n",
    "                elif len(product_desc[i].split()) >= 2 and ' '.join(product_desc[i].split()[0:2]) in unique_brand_set: # Checking whether the first two words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:2])\n",
    "                elif len(product_desc[i].split()) >= 3 and ' '.join(product_desc[i].split()[0:3]) in unique_brand_set: # Checking whether the first three words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:3])\n",
    "                elif len(product_desc[i].split()) >= 4 and ' '.join(product_desc[i].split()[0:4]) in unique_brand_set: # Checking whether the first four words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:4])\n",
    "                elif len(product_desc[i].split()) >= 5 and ' '.join(product_desc[i].split()[0:5]) in unique_brand_set: # Checking whether the first five words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:5])\n",
    "                elif len(product_desc[i].split()) >= 6 and ' '.join(product_desc[i].split()[0:6]) in unique_brand_set: # Checking whether the first six words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:6])\n",
    "                elif len(product_desc[i].split()) >= 7 and ' '.join(product_desc[i].split()[0:5]) in unique_brand_set: # Checking whether the first seven words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:7])\n",
    "                elif len(product_desc[i].split()) >= 8 and ' '.join(product_desc[i].split()[0:5]) in unique_brand_set: # Checking whether the first eight words of the description is a valid brand\n",
    "                    brand_name[i] = ' '.join(product_desc[i].split()[0:8])\n",
    "            else:\n",
    "                continue\n",
    "        return brand_name\n",
    "\n",
    "    '''Defining the function to extract the item description length'''\n",
    "    def count_of_words(text):\n",
    "        '''This function would remove the punctuations, numbers and \n",
    "        stopwords from the text and then converting everything to lowercase\n",
    "        and thereby returning the number of words in the text'''\n",
    "        try:\n",
    "            text = text.replace('\\\\t','') # Removing the tabs\n",
    "            text = text.replace('\\\\r','') # Removing the \\r\n",
    "            text = text.replace('\\\\n','') # Removing the newline character\n",
    "            text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+','',text) # Removing the punctuations\n",
    "            text = re.sub('[0-9]+','',text) # Removing the numbers\n",
    "            # Capturing the clean text in a string and then returning the length of the list for words greater than length of 3        \n",
    "            new_text = ' '.join(word for word in text.split() if word.lower() not in nltk.corpus.stopwords.words('english') and len(word)>3)\n",
    "            # Returning the length of the words in the string new_text\n",
    "            return len(new_text.split())        \n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    '''Defining the function to separate out the category names'''\n",
    "    # Defining a function to separate out the three sub categories\n",
    "\n",
    "    def category_split(category):\n",
    "        '''This function separates out 3 particular sub categories from a category name'''\n",
    "\n",
    "        # Defining a try-except block to check whether the category can be splitted into sub categories or not. The except block\n",
    "        # catches the exception if there is no '/' in the category name and then returns a list with strings No Label repeated 3\n",
    "        # times\n",
    "        try:\n",
    "            return category.split('/')\n",
    "        except:\n",
    "            return (\"No Label\",\"No Label\",\"No Label\")\n",
    "        \n",
    "    '''Preprocessing the product name'''\n",
    "    preprocessed_name = preprocess(X.name.values)\n",
    "    \n",
    "    '''Replacing the brand names with the available brand names in the product names'''\n",
    "    preprocessed_brand_nm = brand_name_replace(X['brand_name'].values, X['name'].values)\n",
    "    \n",
    "    '''Replacing the brand names with the substituted brand names'''\n",
    "    X['brand_name'] = preprocessed_brand_nm\n",
    "    \n",
    "    '''Filling out the missing values in the brand name columns with No Brand Name'''\n",
    "    X[\"brand_name\"] = X[\"brand_name\"].fillna(\"No Brand Name\")\n",
    "    \n",
    "    '''Preprocessing the brand names'''\n",
    "    X[\"preprocessed_brand_nm\"] = preprocess(X.brand_name.values)\n",
    "    \n",
    "    '''Preprocessing the categories after getting the individual categories which are general_cat, subcat_1 and subcat_2'''\n",
    "    general_cat, subcat_1, subcat_2 = X['general_cat'], X['subcat_1'], X['subcat_2'] = zip(*X['category_name'].apply(category_split))\n",
    "\n",
    "    \n",
    "    '''Preprocessing the item description'''\n",
    "    X['preprocessed_item_desc'] = preprocess(X['item_description'].values)\n",
    "    \n",
    "    '''Getting the item_description length of the preprocessed item description'''\n",
    "    item_desc_length = count_of_words(preprocessed_item_desc)\n",
    "    \n",
    "    '''Initializing a new list to store lengths of item descriptions'''\n",
    "    desc_length_ls = []\n",
    "\n",
    "    for item_description in tqdm(X[\"item_description\"].values):\n",
    "        desc_length_ls.append(count_of_words(item_description))\n",
    "\n",
    "    X[\"item_description_Length\"] = desc_length_ls\n",
    "    \n",
    "    '''Preprocessing the general_cat'''\n",
    "    X['general_cat'] = X['general_cat'].str.replace(\" & \",\"_\")\n",
    "    X['general_cat'] = X['general_cat'].str.replace(\" \",\"_\")\n",
    "    X['general_cat'] = X['general_cat'].str.lower()\n",
    "    \n",
    "    '''Preprocessing the subcat_1'''\n",
    "    X['subcat_1'] = X['subcat_1'].str.replace(\" & \",\"_\")\n",
    "    X['subcat_1'] = X['subcat_1'].str.replace(\" \",\"_\")\n",
    "    X['subcat_1'] = X['subcat_1'].str.replace(\"-\",\"_\")\n",
    "    X['subcat_1'] = X['subcat_1'].str.replace(\"\\'s\",\"\")\n",
    "    X['subcat_1'] = X['subcat_1'].str.replace(\"\\(\",\"\")\n",
    "    X['subcat_1'] = X['subcat_1'].str.replace(\"\\)\",\"\")\n",
    "    X['subcat_1'] = X['subcat_1'].str.lower()\n",
    "    \n",
    "    '''Preprocessing the subcat_2'''\n",
    "    X['subcat_2'] = X['subcat_2'].str.replace(\" & \",\"_\")\n",
    "    X['subcat_2'] = X['subcat_2'].str.replace(\" \",\"_\")\n",
    "    X['subcat_2'] = X['subcat_2'].str.replace(\"-\",\"_\")\n",
    "    X['subcat_2'] = X['subcat_2'].str.replace(\"\\'s\",\"\")\n",
    "    X['subcat_2'] = X['subcat_2'].str.replace(\"\\(\",\"\")\n",
    "    X['subcat_2'] = X['subcat_2'].str.replace(\"\\)\",\"\")\n",
    "    X['subcat_2'] = X['subcat_2'].str.lower()\n",
    "    \n",
    "    '''Sentiment Score'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment = []\n",
    "\n",
    "    for sentence in tqdm(test_data.preprocessed_item_desc.values):\n",
    "        ss = sid.polarity_scores(str(sentence))\n",
    "        sentiment.append(ss)\n",
    "        \n",
    "    '''Converting the list of dictionaries into a dataframe'''\n",
    "    sentiment_score_df = pd.DataFrame(sentiment)\n",
    "    \n",
    "    '''Adding the sentiment score to the data'''\n",
    "\n",
    "    X['neg'] = sentiment_score_df['neg']\n",
    "    X['neu'] = sentiment_score_df['neu']\n",
    "    X['pos'] = sentiment_score_df['pos']\n",
    "    X['compound'] = sentiment_score_df['compound']\n",
    "    \n",
    "    '''Concatenating the preprocessed_name and preprocessed_item_desc'''\n",
    "    X[\"name_item_desc\"] = X[\"preprocessed_name\"] + \" \" + X[\"preprocessed_item_desc\"]\n",
    "    \n",
    "    '''Concatenating the preprocessed_brand_nm, general_cat, subcat_1 and subcat_2'''\n",
    "    X[\"concatenated_brcat\"] = X[\"preprocessed_brand_nm\"] + \" \" + X[\"general_cat\"] + \" \" + X[\"subcat_1\"] + \" \" + X[\"subcat_2\"]\n",
    "    \n",
    "    '''Numerical features'''\n",
    "    numerical_features = X.loc[:,['item_condition_id', 'shipping', 'item_description_Length','neg','pos','neu','compound']]\n",
    "\n",
    "    '''Loading the tokenizers'''\n",
    "    Tokenizer_pritem_desc = joblib.load('Tokenizer_pritem_desc.pkl')\n",
    "    Tokenizer_brcat = joblib.load('Tokenizer_brcat.pkl')\n",
    "    \n",
    "    '''Loading the minmaxscaler'''\n",
    "    scaler = joblib.load('minmaxscaler.pkl')\n",
    "    \n",
    "    '''Loading the embedding matrix'''\n",
    "    embedding_matrix = np.load('embedding_matrix.npy')\n",
    "    \n",
    "    '''Scaling the numerical features'''\n",
    "    numerical_features_scaled = scaler.transform(numerical_features.values)\n",
    "    \n",
    "    '''Creating a list for concatenated product name and item description preprocessed'''\n",
    "    pritem_desc_list = []\n",
    "    for text in list(X[\"name_item_desc\"]):\n",
    "        if isinstance(text,float):\n",
    "            text = str(text)\n",
    "            pritem_desc_list.append(text)\n",
    "        else:\n",
    "            pritem_desc_list.append(text)\n",
    "    \n",
    "    '''Creating a list of concatenated brand name and categories'''\n",
    "    brcat_list = []\n",
    "    for text in list(X[\"concatenated_brcat\"]):\n",
    "        if isinstance(text,float):\n",
    "            text = str(text)\n",
    "            brcat_list.append(text)\n",
    "        else:\n",
    "            brcat_list.append(text)\n",
    "    \n",
    "    '''Generating the padded sequence for the concatenated product name and preprocessed_item_desc'''\n",
    "    pritem_desc_sequence = Tokenizer_pritem_desc.texts_to_sequences(pritem_desc_list)\n",
    "    pritem_desc_padded_sequence = pad_sequences(pritem_desc_sequence, padding = 'post', truncating = 'post', maxlen = 154)\n",
    "    \n",
    "    '''Generating the padded sequence for the concatenated brand name and the categories'''\n",
    "    brcat_sequence = Tokenizer_brcat.texts_to_sequences(brcat_list)\n",
    "    brcat_padded_sequence = pad_sequences(brcat_sequence, padding = 'post', truncating = 'post', maxlen = 8)\n",
    "    \n",
    "    ''' Defining the model variables'''\n",
    "    maxlen_pritem_desc = 154\n",
    "    maxlen_brcat = 8\n",
    "    \n",
    "    ''' Defining the model architecture'''\n",
    "    embedding_dim_brcat = 50\n",
    "    num_tokens_brcat = len(Tokenizer_brcat.word_index)+1\n",
    "    embedding_dim_preprocessed_pritemdesc = 300\n",
    "    num_tokens_preprocessed_pritemdesc = len(Tokenizer_pritem_desc.word_index)+1\n",
    "\n",
    "    # Creating the model architecture\n",
    "    Inp1 = Input(shape = (maxlen_pritem_desc,), dtype='int64')\n",
    "    Emb1 = Embedding(input_dim = num_tokens_preprocessed_pritemdesc, output_dim = embedding_dim_preprocessed_pritemdesc, input_length = maxlen_pritem_desc,\n",
    "                    embeddings_initializer = tf.keras.initializers.constant(embedding_matrix), trainable = False)(Inp1)\n",
    "    LSTM_layer_1 = LSTM(units=50, return_sequences = True)(Emb1)\n",
    "\n",
    "    #avgpool = AveragePooling1D(pool_size = 2, strides=2)(Emb1)\n",
    "    #Flatten_1 = Flatten()(avgpool)\n",
    "\n",
    "    Inp2 = Input(shape = (maxlen_brcat,), dtype='int64')\n",
    "    Emb2 = Embedding(input_dim = num_tokens_brcat, output_dim = embedding_dim_brcat, input_length = maxlen_brcat,trainable = True)(Inp2)\n",
    "    LSTM_layer_2 = LSTM(units=50, return_sequences = True)(Emb2)\n",
    "    #Flatten_2 = Flatten()(Emb2)\n",
    "\n",
    "    att_contextvec = Attention()([LSTM_layer_2,LSTM_layer_1])\n",
    "\n",
    "    avgpool = GlobalAveragePooling1D()(att_contextvec)\n",
    "\n",
    "    Flatten_1 = Flatten()(avgpool)\n",
    "\n",
    "    Inp3 = Input(shape = (7,))\n",
    "    Dense_1 = Dense(units = 4, activation = 'relu', kernel_initializer = 'he_normal')(Inp3)\n",
    "    #Dense_2 = Dense(units = 4, activation = 'relu', kernel_initializer = 'he_uniform')(Dense_2)\n",
    "\n",
    "    concat = concatenate([Flatten_1,Dense_1])\n",
    "\n",
    "    BN_1 = BatchNormalization()(concat)\n",
    "\n",
    "    Dense_2 = Dense(units = 16, kernel_initializer = 'he_normal')(BN_1)\n",
    "\n",
    "    Dense_2 = tf.keras.layers.PReLU()(Dense_2)\n",
    "\n",
    "    dropout_1 = Dropout(0.5)(Dense_2)\n",
    "\n",
    "    #BN_2 = BatchNormalization()(dropout_1)\n",
    "\n",
    "    Dense_3 = Dense(units = 8, kernel_initializer = 'he_normal')(dropout_1)\n",
    "\n",
    "    Dense_3 = tf.keras.layers.PReLU()(Dense_3)\n",
    "\n",
    "    dropout_2 = Dropout(0.4)(Dense_3)\n",
    "\n",
    "    Output = Dense(units = 1, activation = 'relu')(dropout_2)\n",
    "\n",
    "    model = Model(inputs = [Inp1, Inp2, Inp3], outputs = [Output])\n",
    "    \n",
    "    '''Loading the best model weights'''\n",
    "    model.load_weights('best_model_weights.hdf5')\n",
    "    \n",
    "    '''Evaluating the best model on the data'''\n",
    "    score = model.evaluate([np.array(pritem_desc_padded_sequence), np.array(brcat_padded_sequence), numerical_features_scaled],Y)\n",
    "    \n",
    "    '''Printing the metric'''\n",
    "    print(\"The rmsle on the data given is : {}\".format(score[1]))\n",
    "    \n",
    "    '''Returning the evaluation metric'''\n",
    "    return score[1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
       "4         0          Complete with certificate of authenticity  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Importing the train data'''\n",
    "\n",
    "training_data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.loc[:,[\"train_id\",\"name\",\"item_condition_id\",\"category_name\",\"brand_name\",\"shipping\",\"item_description\"]]\n",
    "Y = training_data[[\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle = final_metric(X,Y)\n",
    "\n",
    "print(\"The root mean squared logarithmic error on the training data is : {}\".format(rmsle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Razer BlackWidow Chroma Keyboard'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Electronics/Computers & Tablets/Components & Parts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.iloc[1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New with tags. Leather horses. Retail for [rm] each. Stand about a foot high. They are being sold as a pair. Any questions please ask. Free shipping. Just got out of storage'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.iloc[3,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
